# ğŸ¤– LLM QA Module

<div align="center">

![Python](https://img.shields.io/badge/Python-3.11-blue?style=for-the-badge&logo=python&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104-009688?style=for-the-badge&logo=fastapi&logoColor=white)
![Ollama](https://img.shields.io/badge/Ollama-Mistral_Nemo-black?style=for-the-badge&logo=ollama&logoColor=white)
![RAG](https://img.shields.io/badge/RAG-Enabled-purple?style=for-the-badge)

**ğŸ¥ Module de Questions/RÃ©ponses MÃ©dicales avec Intelligence Artificielle**

*RAG Architecture â€¢ Mistral Nemo 12B â€¢ RÃ©ponses contextuelles prÃ©cises*

[Architecture](#-architecture-rag) â€¢
[FonctionnalitÃ©s](#-fonctionnalitÃ©s) â€¢
[API](#-api-endpoints) â€¢
[Configuration](#-configuration)

</div>

---

## ğŸ¯ PrÃ©sentation

**LLM QA Module** est le cerveau de DocQA. Il permet aux professionnels de santÃ© de poser des questions en langage naturel sur les dossiers patients et d'obtenir des rÃ©ponses prÃ©cises, sourcÃ©es et contextuelles grÃ¢ce Ã  l'architecture RAG (Retrieval-Augmented Generation).

```
â“ Question â†’ ğŸ” Recherche Documents â†’ ğŸ§  LLM Analysis â†’ ğŸ’¬ RÃ©ponse SourcÃ©e
```

---

## ğŸ—ï¸ Architecture RAG

```
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚         QUESTION UTILISATEUR     â”‚
                         â”‚  "Quels traitements prescrits?" â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                  â”‚                  â”‚
                    â–¼                  â–¼                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Query Router   â”‚ â”‚ Query Expansion â”‚ â”‚ Medical Terms   â”‚
         â”‚  (Filtrage)     â”‚ â”‚ (Synonymes)     â”‚ â”‚ Enhancement     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                   â”‚                   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚      INDEXEUR SÃ‰MANTIQUE        â”‚
                         â”‚   Recherche Vectorielle Top-K   â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚         RERANKING (LLM)         â”‚
                         â”‚    Tri par pertinence rÃ©elle    â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚        CONTEXT BUILDER          â”‚
                         â”‚  Construction du contexte RAG   â”‚
                         â”‚     (max 12000 caractÃ¨res)      â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚       MISTRAL NEMO 12B          â”‚
                         â”‚    GÃ©nÃ©ration de la rÃ©ponse     â”‚
                         â”‚   avec citations des sources    â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚    RÃ‰PONSE + SOURCES + SCORE    â”‚
                         â”‚  "Selon [Source 1], le patient  â”‚
                         â”‚   est traitÃ© par..." (0.89)     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ FonctionnalitÃ©s

### ğŸ§  ModÃ¨le LLM : Mistral Nemo 12B Instruct

| CaractÃ©ristique | Valeur |
|-----------------|--------|
| **ParamÃ¨tres** | 12 milliards |
| **Context Window** | 128K tokens (16K utilisÃ©s) |
| **Langue** | FranÃ§ais excellent |
| **Type** | Instruction-tuned |
| **MÃ©moire** | ~8GB VRAM / ~16GB RAM |

### âš¡ ParamÃ¨tres OptimisÃ©s pour le MÃ©dical

```python
# Configuration optimale pour rÃ©ponses mÃ©dicales prÃ©cises
LLM_TEMPERATURE = 0.05      # TrÃ¨s faible â†’ rÃ©ponses factuelles
LLM_TOP_P = 0.85            # FocalisÃ© sur les tokens probables
LLM_TOP_K = 30              # LimitÃ© pour plus de dÃ©terminisme
LLM_NUM_CTX = 16384         # Grand contexte pour RAG
LLM_REPEAT_PENALTY = 1.15   # Ã‰vite les rÃ©pÃ©titions
```

### ğŸ” Pipeline RAG Complet

| Ã‰tape | Description | Config |
|-------|-------------|--------|
| 1ï¸âƒ£ **Query Expansion** | Enrichissement avec synonymes mÃ©dicaux | Auto |
| 2ï¸âƒ£ **Vector Search** | Recherche sÃ©mantique via Indexeur | Top 5 |
| 3ï¸âƒ£ **Reranking** | Re-scoring par pertinence LLM | Top 3 |
| 4ï¸âƒ£ **Context Building** | Construction contexte structurÃ© | 12K chars |
| 5ï¸âƒ£ **Generation** | GÃ©nÃ©ration rÃ©ponse Mistral | Streaming |
| 6ï¸âƒ£ **Confidence** | Calcul score de confiance | 0-1 |

### ğŸ“Š Scoring de Confiance

```
Score basÃ© sur:
â”œâ”€â”€ ğŸ“š Nombre de sources citÃ©es (30%)
â”œâ”€â”€ ğŸ“ Longueur de la rÃ©ponse (20%)
â”œâ”€â”€ ğŸ¥ Termes mÃ©dicaux utilisÃ©s (25%)
â””â”€â”€ âš ï¸ Absence d'incertitude (25%)
```

---

## ğŸ› ï¸ API Endpoints

### `POST /api/qa/ask`

Pose une question avec contexte RAG.

```bash
curl -X POST "http://localhost:8004/api/qa/ask" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Quels sont les traitements prescrits au patient?",
    "patientId": "P12345",
    "maxContextDocs": 5
  }'
```

**Response:**
```json
{
  "answer": "D'aprÃ¨s les documents mÃ©dicaux analysÃ©s, le patient bÃ©nÃ©ficie des traitements suivants:\n\n1. **DonÃ©pÃ©zil 5mg/jour** - prescrit pour la suspicion de maladie d'Alzheimer au stade prodromal\n2. **Bilan orthophonique** - pour stimulation cognitive\n\nUn suivi en hÃ´pital de jour gÃ©riatrique est Ã©galement prÃ©vu pour un bilan complet.",
  "confidence": 0.89,
  "sources": [
    {
      "index": 1,
      "filename": "Consultation_Neuro_001.pdf",
      "excerpt": "DÃ©buter traitement par donÃ©pÃ©zil 5 mg/jour..."
    }
  ],
  "processingTimeMs": 15234,
  "queryId": "qa-uuid-12345"
}
```

### `POST /api/qa/extract`

Extrait des informations structurÃ©es.

```bash
curl -X POST "http://localhost:8004/api/qa/extract" \
  -H "Content-Type: application/json" \
  -d '{
    "documentId": "123",
    "extractionType": "pathologies"
  }'
```

**Types d'extraction:**
- `pathologies` - Maladies et diagnostics
- `traitements` - MÃ©dicaments et posologies
- `antecedents` - Historique mÃ©dical

### `GET /api/qa/history/{sessionId}`

RÃ©cupÃ¨re l'historique de chat.

```json
{
  "sessionId": "session-uuid",
  "messages": [
    {
      "role": "user",
      "content": "Quels traitements?",
      "timestamp": "2025-12-05T10:30:00Z"
    },
    {
      "role": "assistant", 
      "content": "Le patient...",
      "sources": [...],
      "timestamp": "2025-12-05T10:30:15Z"
    }
  ]
}
```

### `GET /health`

```json
{
  "status": "healthy",
  "llm": {
    "model": "mistral-nemo",
    "status": "connected",
    "responseTimeMs": 234
  },
  "indexeur": "connected",
  "version": "1.0.0"
}
```

---

## âš™ï¸ Configuration

### Variables d'Environnement

```env
# ğŸ”§ Service
SERVICE_NAME=LLMQAModule
SERVICE_PORT=8004
SERVICE_HOST=0.0.0.0

# ğŸ¤– Ollama / Mistral Nemo
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral-nemo
USE_LOCAL_LLM=true

# ğŸ›ï¸ ParamÃ¨tres LLM (OptimisÃ©s MÃ©dical)
LLM_TEMPERATURE=0.05
LLM_TOP_P=0.85
LLM_TOP_K=30
LLM_NUM_CTX=16384
LLM_REPEAT_PENALTY=1.15

# ğŸ” RAG Configuration
RAG_TOP_K_RESULTS=5
RAG_CHUNK_SIZE=1024
RAG_CHUNK_OVERLAP=100
RAG_SIMILARITY_THRESHOLD=0.25
MAX_CONTEXT_LENGTH=12000

# ğŸ”„ Reranking
USE_RERANKING=true
RERANK_TOP_K=3

# ğŸ”— Services
INDEXEUR_SERVICE_URL=http://indexeur-semantique:8003
AUDIT_SERVICE_URL=http://audit-logger:8006
```

---

## ğŸ“¦ Installation

### PrÃ©requis: Ollama + Mistral Nemo

```bash
# 1. Installer Ollama
# Windows: https://ollama.ai/download
# Linux: curl -fsSL https://ollama.ai/install.sh | sh

# 2. TÃ©lÃ©charger Mistral Nemo (7.1GB)
ollama pull mistral-nemo

# 3. VÃ©rifier
ollama list
# NAME           SIZE
# mistral-nemo   7.1GB
```

### ğŸ³ Docker (RecommandÃ©)

```bash
# Depuis la racine du projet
docker-compose up -d llm-qa-module
```

> âš ï¸ Le conteneur accÃ¨de Ã  Ollama via `host.docker.internal:11434`

### ğŸ’» Local

```bash
# 1. Environnement
cd microservices/llm-qa-module
python -m venv venv
source venv/bin/activate

# 2. DÃ©pendances
pip install -r requirements.txt

# 3. Configuration
cp .env.example .env

# 4. Lancer
python app.py
```

---

## ğŸ“ Structure du Projet

```
llm-qa-module/
â”œâ”€â”€ ğŸ“„ app.py                 # Point d'entrÃ©e FastAPI
â”œâ”€â”€ âš™ï¸ config.py              # Configuration Pydantic
â”œâ”€â”€ ğŸ“‹ requirements.txt       # DÃ©pendances
â”œâ”€â”€ ğŸ³ Dockerfile            
â”‚
â””â”€â”€ ğŸ“‚ src/
    â”œâ”€â”€ ğŸ“‚ api/
    â”‚   â””â”€â”€ routes.py         # Endpoints REST
    â”‚
    â”œâ”€â”€ ğŸ“‚ services/
    â”‚   â”œâ”€â”€ qa_service.py     # Logique RAG principale
    â”‚   â”œâ”€â”€ llm_client.py     # Client Ollama
    â”‚   â”œâ”€â”€ indexer_client.py # Client Indexeur
    â”‚   â””â”€â”€ prompt_builder.py # Construction prompts
    â”‚
    â””â”€â”€ ğŸ“‚ models/
        â””â”€â”€ schemas.py        # Pydantic models
```

---

## ğŸ¯ Prompt Engineering

### Prompt SystÃ¨me MÃ©dical

```python
SYSTEM_PROMPT = """
Tu es un assistant mÃ©dical expert francophone spÃ©cialisÃ© dans 
l'analyse de dossiers patients. Tu fournis des rÃ©ponses prÃ©cises, 
structurÃ©es et professionnelles basÃ©es uniquement sur les documents 
fournis.

RÃˆGLES:
- Cite toujours tes sources [Source X]
- Utilise la terminologie mÃ©dicale appropriÃ©e  
- Structure ta rÃ©ponse avec des listes si pertinent
- Si l'information n'est pas disponible, dis-le clairement
- Ne fais JAMAIS de diagnostic non mentionnÃ© dans les documents
"""
```

### Template de RÃ©ponse

```
CONTEXTE MÃ‰DICAL:
{documents_context}

QUESTION: {question}

RÃ©ponds de maniÃ¨re professionnelle et structurÃ©e en citant les sources.
```

---

## ğŸ“Š Performance

| MÃ©trique | Valeur |
|----------|--------|
| â±ï¸ Temps gÃ©nÃ©ration | 15-30s |
| ğŸ“Š Tokens/seconde | 30-50 |
| ğŸ’¾ MÃ©moire GPU | ~8GB |
| ğŸ“ Context max | 16K tokens |
| ğŸ¯ PrÃ©cision RAG | ~85% |

---

## ğŸ› Troubleshooting

### Ollama non connectÃ©

```bash
# VÃ©rifier qu'Ollama tourne
curl http://localhost:11434/api/tags

# Relancer Ollama
ollama serve
```

### Timeout sur les requÃªtes

```python
# Augmenter le timeout (config.py ou env)
LLM_TIMEOUT=180  # 3 minutes pour requÃªtes complexes
```

### MÃ©moire insuffisante

```bash
# Mode CPU (plus lent mais fonctionne)
# Ollama utilise automatiquement le CPU si pas de GPU

# VÃ©rifier la RAM disponible (besoin ~16GB)
```

---

## ğŸ”— Flux RAG Complet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend  â”‚â”€â”€â”€â–ºâ”‚ LLM-QA-Moduleâ”‚â”€â”€â”€â–ºâ”‚   Indexeur    â”‚â”€â”€â”€â–ºâ”‚  Mistral   â”‚
â”‚ (Question) â”‚    â”‚              â”‚    â”‚  (Recherche)  â”‚    â”‚   Nemo     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                  â”‚                   â”‚                   â”‚
      â”‚                  â”‚                   â”‚                   â”‚
      â–¼                  â–¼                   â–¼                   â–¼
   "Quels          Query + Filter       Top 5 Docs         RÃ©ponse
  traitements?"    par patientId        Pertinents        StructurÃ©e
```

---

<div align="center">

**Fait avec â¤ï¸ pour DocQA**

*L'IA au service des professionnels de santÃ©*

ğŸ§  **Mistral Nemo 12B** | ğŸ” **RAG Architecture** | ğŸ¥ **MÃ©dical-First**

</div>
