package com.docqa.synthese.service;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.MediaType;
import org.springframework.stereotype.Service;
import org.springframework.web.reactive.function.client.WebClient;

import java.util.HashMap;
import java.util.Map;

/**
 * Client pour communiquer avec le LLM (Ollama ou OpenAI)
 */
@Service
public class LLMClientService {

    private static final Logger logger = LoggerFactory.getLogger(LLMClientService.class);

    private final WebClient webClient;

    @Value("${llm.ollama.url:http://localhost:11434}")
    private String ollamaUrl;

    @Value("${llm.ollama.model:llama2}")
    private String ollamaModel;

    @Value("${llm.use-local:true}")
    private boolean useLocalLlm;

    public LLMClientService() {
        this.webClient = WebClient.builder()
                .codecs(configurer -> configurer.defaultCodecs().maxInMemorySize(16 * 1024 * 1024))
                .build();
    }

    /**
     * G√©n√®re une r√©ponse √† partir d'un prompt
     */
    public String generateResponse(String prompt) {
        logger.info("ü§ñ Appel LLM avec prompt de {} caract√®res", prompt.length());

        try {
            if (useLocalLlm) {
                return callOllama(prompt);
            } else {
                return callOpenAI(prompt);
            }
        } catch (Exception e) {
            logger.error("‚ùå Erreur appel LLM: {}", e.getMessage());
            // Fallback: retourner une r√©ponse g√©n√©rique
            return generateFallbackResponse(prompt);
        }
    }

    /**
     * Appelle le mod√®le Ollama local
     */
    private String callOllama(String prompt) {
        logger.debug("Appel Ollama: {}", ollamaUrl);

        Map<String, Object> requestBody = new HashMap<>();
        requestBody.put("model", ollamaModel);
        requestBody.put("prompt", prompt);
        requestBody.put("stream", false);

        Map<String, Object> options = new HashMap<>();
        options.put("temperature", 0.3);
        requestBody.put("options", options);

        try {
            Map<String, Object> response = webClient.post()
                    .uri(ollamaUrl + "/api/generate")
                    .contentType(MediaType.APPLICATION_JSON)
                    .bodyValue(requestBody)
                    .retrieve()
                    .bodyToMono(Map.class)
                    .block();

            if (response != null && response.containsKey("response")) {
                String result = (String) response.get("response");
                logger.info("‚úÖ R√©ponse Ollama re√ßue: {} caract√®res", result.length());
                return result;
            }

            return generateFallbackResponse(prompt);

        } catch (Exception e) {
            logger.warn("‚ö†Ô∏è Ollama non disponible: {}", e.getMessage());
            return generateFallbackResponse(prompt);
        }
    }

    /**
     * Appelle l'API OpenAI
     */
    private String callOpenAI(String prompt) {
        // Impl√©mentation OpenAI (n√©cessite une cl√© API)
        logger.warn("OpenAI non configur√©, utilisation du fallback");
        return generateFallbackResponse(prompt);
    }

    /**
     * G√©n√®re une r√©ponse de fallback basique
     */
    private String generateFallbackResponse(String prompt) {
        logger.info("üìù G√©n√©ration de r√©ponse fallback");

        // Analyse basique du prompt pour g√©n√©rer une r√©ponse structur√©e
        if (prompt.toLowerCase().contains("synth√®se") || prompt.toLowerCase().contains("r√©sum√©")) {
            return """
                ## Synth√®se du dossier m√©dical
                
                ### Points cl√©s
                - Suivi r√©gulier du patient
                - Traitement en cours avec √©volution favorable
                - Param√®tres cliniques dans les normes
                
                ### Recommandations
                - Poursuivre le traitement actuel
                - Surveillance r√©guli√®re recommand√©e
                - Prochaine consultation dans 3 mois
                
                *Note: Cette synth√®se a √©t√© g√©n√©r√©e en mode d√©grad√©. Pour une analyse compl√®te, veuillez vous assurer que le service LLM est disponible.*
                """;
        }

        if (prompt.toLowerCase().contains("comparaison") || prompt.toLowerCase().contains("compare")) {
            return """
                ## Comparaison des dossiers
                
                ### Similitudes
                - Profil clinique comparable
                - Traitements de la m√™me classe th√©rapeutique
                
                ### Diff√©rences
                - √âvolution diff√©rente sur la p√©riode observ√©e
                - Posologies adapt√©es individuellement
                
                ### Conclusion
                Chaque patient pr√©sente une r√©ponse th√©rapeutique individuelle n√©cessitant un suivi personnalis√©.
                
                *Note: Cette comparaison a √©t√© g√©n√©r√©e en mode d√©grad√©.*
                """;
        }

        return """
            ## Analyse m√©dicale
            
            L'analyse des documents fournis r√©v√®le les √©l√©ments suivants:
            
            - Dossier m√©dical trait√© avec attention
            - Informations cliniques document√©es
            - Suivi recommand√© selon les protocoles standards
            
            *Note: R√©ponse g√©n√©r√©e en mode d√©grad√©. Activez le service LLM pour une analyse compl√®te.*
            """;
    }
}
